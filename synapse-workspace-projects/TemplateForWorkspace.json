{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "synapse-workspace-projects"
		},
		"synapse-workspace-projects-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'synapse-workspace-projects-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:synapse-workspace-projects.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"synapse-workspace-projects-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://projectsstore1.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/calebspool')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapse-workspace-projects-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('synapse-workspace-projects-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapse-workspace-projects-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('synapse-workspace-projects-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SelfHostIntegrationRuntime-2')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "SelfHosted",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CreateDB')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- Delete database if it exists\n\nIF EXISTS (SELECT name from sys.databases WHERE name = 'MyProjectsDB')\nDROP DATABASE MyProjectsDB;\n\n-- Create database\nCREATE DATABASE MyProjectsDB;\n\n/*\n    A master key in SQL is used to encrypt sensitive data, such as credentials, to ensure secure access. \n    It is required for operations like creating database-scoped credentials. \n    By creating a master key, you enable encryption for securely storing and accessing credentials \n    or other sensitive information within the database.\n*/\n\n-- Create the master key for encryption\nCREATE MASTER KEY ENCRYPTION BY PASSWORD = 'MyS3cureP@ssw0rd!';\n\n/*\n    A SAS token grants time-limited access to specific Azure storage resources \n    without exposing the account key,\n    allowing external apps or users to access resources with defined permissions.\n*/\n\n-- Create a database-scoped credential for accessing datalake\nCREATE DATABASE SCOPED CREDENTIAL myStorageCred\nWITH IDENTITY = 'SHARED ACCESS SIGNATURE',\nSECRET = '&sr=c&sig=eb9Qee0ACuFPNGl6r0lcpPPyxc87u9sZHp%2FaZBBU6kw%3D';\n\n\nCREATE EXTERNAL DATA SOURCE ExternalDataLakeStoreProj\nWITH (\n    LOCATION = 'https://projectsstore1.blob.core.windows.net/synapse-proj-files/'\n    CREDENTIAL = myStorageCred\n);\n\n\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "/*\n2.1 Create External Data Source\n\n    The external data source connects Synapse Analytics with your Azure Data Lake Storage.\n    We need to specify the location and credentials.\n\n*/\n\n-- SELECT * FROM sys.database_scoped_credentials;\n\n\nCREATE EXTERNAL DATA SOURCE ExternalDataLakeStore\nWITH (\n\n    LOCATION = 'https://projectsstore1.blob.core.windows.net/synapse-proj-files/',\n    CREDENTIAL = myStorageCred\n);\n\n\n/*\n2.2 Create External File Format\n\n    Now, create the file format to specify the format of the CSV files (Delimited format for CSV).\n\n*/\n\n-- Create an external file format for DELIMITED CSV files using FORMAT_OPTIONS\nCREATE EXTERNAL FILE FORMAT CsvFormat\nWITH (\n    FORMAT_TYPE = DELIMITEDTEXT,\n    FORMAT_OPTIONS (\n        FIELD_TERMINATOR = ',',\n        FIRST_ROW = 2 -- Skip the header row\n    )\n);\n\n-- DROP EXTERNAL FILE FORMAT CsvFormat\n\n\n/* 2.3 Create External Tables\n\nCreate external tables for both the matches and the deliveries datasets using the defines schema.\n\n*/\nCREATE EXTERNAL TABLE matches (\n    id INT,\n    season NVARCHAR(50),\n    city NVARCHAR(50),\n    date DATE,\n    match_type NVARCHAR(50),\n    player_of_match NVARCHAR(100),\n    venue NVARCHAR(100),\n    team1 NVARCHAR(50),\n    team2 NVARCHAR(50),\n    toss_winner NVARCHAR(50),\n    toss_decision NVARCHAR(50),\n    winner NVARCHAR(50),\n    result NVARCHAR(50),\n    result_margin INT,\n    target_runs NVARCHAR(50),\n    target_overs NVARCHAR(50),\n    super_over NVARCHAR(50),\n    method NVARCHAR(50),\n    umpire1 NVARCHAR(50),\n    umpire2 NVARCHAR(50)\n) \nWITH (\n    LOCATION = 'matches/*.csv',\n    DATA_SOURCE = ExternalDataLakeStore,\n    FILE_FORMAT = CsvFormat\n);\n\n\nCREATE EXTERNAL TABLE deliveries(\n    match_id INT,\n    inning INT,\n    batting_team NVARCHAR(100),\n    bowling_team NVARCHAR(100),\n    over_ INT,\n    ball INT,\n    batter NVARCHAR(100),\n    bowler NVARCHAR(100),\n    non_striker NVARCHAR(100),\n    batsman_runs INT,\n    extra_runs INT,\n    total_runs INT,\n    extras_type NVARCHAR(100),\n    is_wicket INT,\n    player_dismissed NVARCHAR(100),\n    dismissal_kind NVARCHAR(100),\n    fielder NVARCHAR(100)\n)\nWITH (\n    LOCATION = 'deliveries/*.csv',\n    DATA_SOURCE = ExternalDataLakeStore,\n    FILE_FORMAT = CsvFormat\n);\n\n\nSELECT * FROM sys.external_data_sources;\n\nSELECT TOP 10 * FROM matches;\n\n\n-- DROP EXTERNAL DATA SOURCE ExternalDataLakeStore\n-- DROP EXTERNAL TABLE matches \n-- DROP EXTERNAL TABLE deliveries;\n\n-- https://projectsstore1.blob.core.windows.net/synapse-proj-files\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "MyProjectsDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ipl-notebook')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "calebspool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "6b42bb97-6bca-4721-a51b-1b87328142da"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b9c2f884-2c91-48d5-a5e9-8e4a26af9d68/resourceGroups/rg-db-projects/providers/Microsoft.Synapse/workspaces/synapse-workspace-projects/bigDataPools/calebspool",
						"name": "calebspool",
						"type": "Spark",
						"endpoint": "https://synapse-workspace-projects.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/calebspool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#### **Start spark session**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"spark = SparkSession.builder.appName(\"IPL Data Analysis\").getOrCreate()\r\n",
							"spark"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#### **Data Loading**\r\n",
							"\r\n",
							"This two datasets - **deliveries** and **matches** from Azure Data Lake Storage into Spark DataFrames. The data is stored in CSV format, and the `header=True` parameter ensures that column names from the files are retained.\r\n",
							"\r\n",
							"###### ETL Pipeline Phase\r\n",
							"This code represents the **Extract** phase of the ETL (Extract, Transform, Load) pipeline. Data is being extracted from a cloud-based storage system (Azure Data Lake) into Spark for further processing.\r\n",
							"\r\n",
							"###### Data Source\r\n",
							"The data is sourced from an Azure Data Lake Storage Gen2 container, specifically from the following paths:\r\n",
							"- Deliveries dataset: `abfss://synapse-proj-files@projectsstore1.dfs.core.windows.net/deliveries/deliveries.csv`\r\n",
							"- Matches dataset: `abfss://synapse-proj-files@projectsstore1.dfs.core.windows.net/matches/matches.csv`\r\n",
							"\r\n",
							"This approach is typical in cloud-based big data engineering workflows, where raw data is extracted for further transformation and analysis.\r\n",
							"\r\n",
							"\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"deliveries = spark.read.load('abfss://synapse-proj-files@projectsstore1.dfs.core.windows.net/deliveries/deliveries.csv', format='csv'\r\n",
							", header=True\r\n",
							")\r\n",
							"matches = spark.read.load('abfss://synapse-proj-files@projectsstore1.dfs.core.windows.net/matches/matches.csv', format='csv'\r\n",
							", header=True\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas as pd"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/calebspool')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 10
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		}
	]
}